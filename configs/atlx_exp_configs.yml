
# Experiment_1
# ------------
exp_1: #-v 75.19% -t 82.43%
  # Description
  description: >
    Baseline
  # Hyper parameters
  hyperparams: &h1
    epochs: 35
    batch_size: 25
    shuffle: True
    cell_num: 300
    layer_num: 1
    lx_dim: 300
    dropout_keep_prob: 0.5
    optimizer: adagrad
    learning_rate: 0.01
    initial_accumulator_value: 0.0000000001 #1e-10
    lambda: 0.001 # l2
    initializer: random_uniform
    seed: 4
    minval: -0.01
    maxval: 0.01
    #att_reg: False
    #epsilon: 1
    #concat_emb_lx:
    #lx_mode:
    #mgerge_mode:
    #lx_activation: False
    #attention_with_lx: False

exp_2:
  description: >
    Test linear add
  hyperparams:
    <<: *h1
    lx_mode: linear
    merge_mode: add

exp_3:
  description: >
    Test att add
  hyperparams: &h3
    <<: *h1
    lx_mode: att
    merge_mode: add

exp_3-1:
  description: >
    Add lx into attention (alpha) input
  hyperparams:
    <<: *h3
    attention_with_lx: True

exp_4:
  description: >
    Test att concat
  hyperparams: &h4
    <<: *h1
    lx_mode: att
    merge_mode: concat

exp_4-1:
  description: >
    Try lower dx with att concat
  hyperparams:
    <<: *h4
    lx_dim: 5

exp_5:
  description: >
    Test double att
  hyperparams: &h5
    <<: *h1
    lx_mode: att
    merge_mode: att

exp_5-1:
  description: >
    Same as exp_5, add tanh() to h*', requrie manual interventiono on the code
  hyperparams:
    <<: *h5

exp_6:
  description: >
    Naive lexicon concat with embedding
  hyperparams:
    <<: *h1
    concat_emb_lx: True

exp_7:
  description: >
    based on exp_3, att-add with concatenated embedding and lx
  hyperparams:
    <<: *h3
    concat_emb_lx: True

exp_8:
  description: >
    based on exp_1 baseline, explore attention regularization
  hyperparams:
    <<: *h1
    att_reg: True
    epsilon: 0.01

exp_9:
  description: >
    exp_3 + exp_8
  hyperparams:
    <<: *h3
    att_reg: True
    epsilon: 0.1

exp_10:
  description: >
    based on exp_3, explore lexicon attention enhancement
  hyperparams:
    <<: *h3
    att_enh: True
    gamma: 0.1

exp_10-1:
  description: >
    attention enhancement with baseline
  hyperparams:
    <<: *h1
    att_enh: True
    gamma: 1

exp_11:
  description: >
    based on exp_1 baseline, explore attention lmax reg
  hyperparams:
    <<: *h1
    att_lmax: True
    epsilon: 0.08

exp_12:
  description: >
    based on exp_1 baseline, explore attention ent reg
  hyperparams:
    <<: *h1
    att_ent: True
    epsilon: 0.025 # [0.025, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]

exp_13:
  description: >
    exp_3 + att_ent
  hyperparams:
    <<: *h3
    att_ent: True
    epsilon: 0.00005

exp_14:
  description: >
    based on exp_1 baseline, explore attention std reg
  hyperparams:
    <<: *h1
    att_std: True
    epsilon: [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001]

exp_15:
  description: >
    baseline + att_std_mask
  hyperparams:
    <<: *h1
    att_std_mask: True
    epsilon: 0.001 #[1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001]

exp_16:
  description: >
    baseline + ent_reg, similar as exp_8
  hyperparams:
    <<: *h1
    ent_reg: True
    epsilon: [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001]

exp_17:
  description: >
    baseline + att_ent_mask
  hyperparams:
    <<: *h1
    att_ent_mask: True
    epsilon: 0.025
    # [0.9, 0.8, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1] # -- 0.6
    # [0.22, 0.21, 0.2, 0.19, 0.18, 0.16, 0.14, 0.12, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03] # ++ 0.22
    # [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001]

exp_18:
  description: >
    exp_3 + att_ent_mask
  hyperparams:
    <<: *h3
    att_ent_mask: True
    epsilon: 0.001
    #[0.009, 0.008, 0.007, 0.006, 0.004, 0.003, 0.002]
    #[1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001]

exp_19:
  description: >
    exp_3 + att_std_mask
  hyperparams:
    <<: *h3
    att_std_mask: True
    epsilon: 0.0001 #[1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]