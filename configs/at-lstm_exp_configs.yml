
# Experiment_1
# ------------
exp_1:
  # Description
  description: >
    First experiment after stable build try to reproduce results in paper by Wang et al.
  # Hyper parameters
  hyperparams: &h1
    epochs: 35
    batch_size: 25
    shuffle: True
    cell_num: 300
    layer_num: 1
    dropout_keep_prob: 0.5
    optimizer: adagrad
    learning_rate: 0.01
    lambda: 0.001 # l2
    initializer: random_uniform
    seed: 4
    minval: -0.01
    maxval: 0.01

exp_1.1: #82.43%
  description: >
    try initial_accumulator_value (epsilon) in adagrad.
  hyperparams: &h1-1
    <<: *h1
    initial_accumulator_value: 0.0000000001 #1e-10

exp_1.2: #81.91%
  description: >
    Same as exp_1.1, not using dropout
  hyperparams:
    <<: *h1-1
    dropout_keep_prob: 1

exp_1.3: #82.63% best with adam
  description: >
    Based on exp_1, try use adam setting epsilon to 1e-10
  hyperparams: &h1-3
    <<: *h1
    learning_rate: 0.001
    optimizer: adam
    beta1: 0.9
    dropout_keep_prob: 1



exp_2:
  description: >
    Try use adam with learning rate of 0.001
  hyperparams: &h2
    <<: *h1
    epochs: 25
    learning_rate: 0.001
    optimizer: adam

exp_3: # 82.63%
  description: >
    Use dropout_keep_prob = 1
  hyperparams: &h3
    <<: *h2
    dropout_keep_prob: 1

exp_4: # 82.84%
  description: >
    Same as exp_3 Try a different random seed
  hyperparams:
    <<: *h3
    seed: 42


#exp_2: &2 # best 80.99% after 2nd epoch
#  <<: *1
#  description: >
#    try adam 0.001, reduce_mean(ce + l2)
#  hyperparams: &h2
#    <<: *h1
#    epochs: 25
#    learning_rate: 0.001
#    optimizer: adam
#
#exp_3: # This looks pretty bad
#  description: >
#    same as exp_2, try adam 0.01, reduce_mean(ce + l2)
#  hyperparams:
#    <<: *h2
#    learning_rate: 0.01
#
#exp_4: # Manual change of loss function in script was made to test difference
#  description: >
#    same as exp_2, try adam 0.001, reduce_mean(ce) + l2
#  hyperparams: &h4
#    <<: *h2
#    epochs: 2
#
#exp_5:
#  description: >
#    same as exp_4, not using dropout
#  hyperparams:
#    <<: *h4
#    dropout_keep_prob: 1.0
#
#exp_6:
#  description: >
#    same as exp_4, train on both train and dev, test on test, longer epochs
#  hyperparams:
#    <<: *h4
#    epochs: 25
#    optimizer: adagrad
#    learning_rate: 0.01

