
# Experiment_1
# ------------
exp_1: &1
  # Description
  description: >
    This is an description of this experiment.
  # Hyper parameters
  hyperparams: &h1
    epochs: 35
    random_state: 4
    batch_size: 25
    cell_num: 300
    layer_num: 1
    dropout_keep_prob: 0.5
    epsilon: 0.01
    momentum: 0.9
    learning_rate: 0.01
    lambda: 0.001 # l2
    optimizer: adagrad

exp_2: &2 # best 80.99% after 2nd epoch
  <<: *1
  description: >
    try adam 0.001, reduce_mean(ce + l2)
  hyperparams: &h2
    <<: *h1
    epochs: 25
    learning_rate: 0.001
    optimizer: adam

exp_3: # This looks pretty bad
  description: >
    same as exp_2, try adam 0.01, reduce_mean(ce + l2)
  hyperparams:
    <<: *h2
    learning_rate: 0.01

exp_4: # Manual change of loss function in script was made to test difference
  description: >
    same as exp_2, try adam 0.001, reduce_mean(ce) + l2
  hyperparams:
    <<: *h2
